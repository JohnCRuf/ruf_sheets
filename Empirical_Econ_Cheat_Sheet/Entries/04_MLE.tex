\section{Maximum Likelihood Estimation}

\subsection{Setup}
Let $\{X_i\}_{i=1}^n$ iid with density $f_{\theta_0}$ for some $\theta_0\in\Theta\subset\R^d$.

\textbf{Likelihood:} $\ell_n(\theta)=\prod_{i=1}^n f_\theta(X_i)$.

\textbf{Log-likelihood:} $L_n(\theta)=\frac{1}{n}\sum_{i=1}^n \ln f_\theta(X_i)$.

\textbf{MLE:} $\hat\theta_n\in\arg\max_{\theta\in\Theta} L_n(\theta)$.

\subsection{Consistency}
$L_n(\theta)\pto L(\theta):=\E(\ln f_\theta(X))$. By Jensen, $\theta_0$ uniquely maximizes $L(\theta)$: $\E[\ln(f_\theta/f_{\theta_0})]\leq \ln\E[f_\theta/f_{\theta_0}]=0$.

\subsection{Asymptotic Distribution}
Under regularity conditions:
\begin{align*}
    \sqrt{n}(\hat\theta_{\text{MLE}}-\theta_0)\dto N\!\left(0,\; I(\theta_0)^{-1}\right),
\end{align*}
where $I(\theta_0)=-\E\!\left[\frac{\partial^2}{\partial\theta^2}\ln f_{\theta_0}(Y|X)\right]$ is the \textbf{Fisher Information}. This variance is optimal: no regular estimator can achieve a smaller asymptotic variance.

\subsection{Example: Normal}
$X_i\sim N(\mu,\sigma^2)$, $\theta=(\mu,\sigma^2)$ unknown.
\begin{align*}
    L_n(\theta) = -\frac{1}{2}\ln(2\pi)-\frac{1}{2}\ln\sigma^2 - \frac{1}{2n\sigma^2}\sum_{i=1}^n(X_i-\mu)^2.
\end{align*}
FOCs yield $\hat\mu = \bar{X}_n$, $\hat\sigma^2 = \frac{1}{n}\sum(X_i-\bar{X}_n)^2$.

\subsection{Example: Bernoulli}
$X_i\sim\text{Bernoulli}(\theta)$. Log-likelihood: $L_n = \bar{X}_n\ln\theta + (1-\bar{X}_n)\ln(1-\theta)$. FOC: $\hat\theta = \bar{X}_n$. Log-likelihood is concave, so FOC suffices.

\subsection{Conditional MLE}
If $Y|X$ has density $f_\theta(y|x)$: $\ell_n(\theta)=\prod_{i} f_\theta(Y_i|X_i)$. When $Y|X\sim N(X'\beta,\sigma^2)$, CMLE minimizes $\sum(Y_i-X_i'\beta)^2$ $=$ OLS.
